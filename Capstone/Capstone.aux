\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{??}
\citation{codpy}
\citation{sutton2018}
\citation{watkins1992}
\citation{silver2014}
\citation{vanhasselt2016}
\citation{mnih2015}
\citation{ormoneit2002}
\citation{engel2003}
\citation{Xu2007}
\citation{SF2023}
\citation{PLF-JMM-Wilmott}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{Introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{codpy}
\citation{codpy}
\citation{LeMeMi:2024}
\citation{DQN}
\citation{PPO}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{Background}{{2}{2}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{2}{subsection.2.1}\protected@file@percent }
\newlabel{Reinforcement-learning}{{2.1}{2}{Reinforcement Learning}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}General Framework for RL}{2}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{General framework for RL}{{2.1.1}{2}{General Framework for RL}{subsubsection.2.1.1}{}}
\newlabel{tran}{{2.1}{3}{General Framework for RL}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Bellman Equations}{3}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{Bellman Equations}{{2.1.2}{3}{Bellman Equations}{subsubsection.2.1.2}{}}
\newlabel{valfun_bell}{{2.2}{3}{Bellman Equations}{equation.2.2}{}}
\newlabel{sta_act_valfun}{{2.3}{3}{Bellman Equations}{equation.2.3}{}}
\newlabel{opt_valfun}{{2.4}{3}{Bellman Equations}{equation.2.4}{}}
\newlabel{opt_sta_act_valfun}{{2.5}{4}{Bellman Equations}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Algorithms}{4}{subsection.2.2}\protected@file@percent }
\newlabel{Algorithms}{{2.2}{4}{Algorithms}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Q-learning}{4}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{Q-learning}{{2.2.1}{4}{Q-learning}{subsubsection.2.2.1}{}}
\newlabel{iter_bell}{{2.6}{4}{Q-learning}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Policy Gradient}{4}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{Policy Gradient}{{2.2.2}{4}{Policy Gradient}{subsubsection.2.2.2}{}}
\newlabel{PG_rule}{{2.7}{4}{Policy Gradient}{equation.2.7}{}}
\newlabel{eq:advant1}{{2.8}{4}{Policy Gradient}{equation.2.8}{}}
\newlabel{eq:advant2}{{2.9}{4}{Policy Gradient}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}HJB Equation}{5}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{HJB}{{2.2.3}{5}{HJB Equation}{subsubsection.2.2.3}{}}
\newlabel{HJBEq}{{2.10}{5}{HJB Equation}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Heuristic-Controlled Learning}{5}{subsubsection.2.2.4}\protected@file@percent }
\newlabel{Heuristic-Controlled Learning}{{2.2.4}{5}{Heuristic-Controlled Learning}{subsubsection.2.2.4}{}}
\newlabel{HCEq}{{2.13}{5}{Heuristic-Controlled Learning}{equation.2.13}{}}
\newlabel{hc_model}{{2.14}{5}{Heuristic-Controlled Learning}{equation.2.14}{}}
\citation{BTA}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Kernel Reminder}{6}{subsection.2.3}\protected@file@percent }
\newlabel{Kernel Reminder}{{2.3}{6}{Kernel Reminder}{subsection.2.3}{}}
\newlabel{Hk}{{2.16}{6}{Kernel-Based Function Approximation}{equation.2.16}{}}
\newlabel{FIT}{{2.17}{6}{Kernel-Based Function Approximation}{equation.2.17}{}}
\newlabel{OP}{{2.18}{6}{Operator View and Gradient Estimation}{equation.2.18}{}}
\newlabel{nabla}{{2.19}{6}{Operator View and Gradient Estimation}{equation.2.19}{}}
\citation{Cuturi:2013}
\citation{LeMeMi:2024}
\newlabel{pi_k}{{2.20}{7}{Softmax Policies and Their Derivatives}{equation.2.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Kernel RL Algorithms}{7}{section.3}\protected@file@percent }
\newlabel{Kernel RL Algorithms}{{3}{7}{Kernel RL Algorithms}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Kernel RL framework}{7}{subsection.3.1}\protected@file@percent }
\newlabel{Kernel RL framework}{{3.1}{7}{Kernel RL framework}{subsection.3.1}{}}
\newlabel{eq:update_s_main}{{3.1}{7}{Reward Regressor}{equation.3.1}{}}
\citation{LeMeMi:2024}
\newlabel{eq:update_q_main}{{3.2}{8}{Estimating the value functions $V^\pi $ and $Q^\pi $}{equation.3.2}{}}
\newlabel{eq:BE}{{3.3}{8}{Estimating the value functions $V^\pi $ and $Q^\pi $}{equation.3.3}{}}
\newlabel{eq:BE2}{{3.4}{8}{Estimating the value functions $V^\pi $ and $Q^\pi $}{equation.3.4}{}}
\newlabel{eq:BE3}{{3.5}{8}{Estimating the value functions $V^\pi $ and $Q^\pi $}{equation.3.5}{}}
\newlabel{eq:BE4}{{3.6}{8}{Estimating the value functions $V^\pi $ and $Q^\pi $}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Kernel Q-Learning}{8}{subsection.3.2}\protected@file@percent }
\newlabel{Kernel Q-Learning}{{3.2}{8}{Kernel Q-Learning}{subsection.3.2}{}}
\newlabel{qbell}{{3.7}{9}{Kernel Q-Learning}{equation.3.7}{}}
\newlabel{eq:bel_res}{{3.8}{9}{Kernel Q-Learning}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Kernel-Based Q-Value Gradient Estimation}{9}{subsection.3.3}\protected@file@percent }
\newlabel{Kernel-Based Q-Value Gradient Estimation}{{3.3}{9}{Kernel-Based Q-Value Gradient Estimation}{subsection.3.3}{}}
\newlabel{gradientBE}{{3.11}{9}{Estimating the Derivative of the Value Function with Respect to the Policy}{equation.3.11}{}}
\newlabel{gradientBE2}{{3.13}{10}{Gradient of the State Value Function $V^\pi $}{equation.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Kernel Actor-Critic with Bellman Residual Advantage}{10}{subsection.3.4}\protected@file@percent }
\newlabel{AC}{{3.4}{10}{Kernel Actor-Critic with Bellman Residual Advantage}{subsection.3.4}{}}
\newlabel{UPDATEAC}{{3.14}{10}{Actor (Policy Update Using Bellman Residual Advantage)}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Kernelized Non-Parametric HJB}{10}{subsection.3.5}\protected@file@percent }
\newlabel{KHJB}{{3.5}{10}{Kernelized Non-Parametric HJB}{subsection.3.5}{}}
\citation{LeMeMi:2024}
\citation{LeMe:2017}
\newlabel{HJB}{{3.17}{11}{HJB-Modified Value Function Approximation}{equation.3.17}{}}
\newlabel{HJBMAT}{{3.18}{11}{Matrix Representation and Transition Operator}{equation.3.18}{}}
\newlabel{NW}{{3.19}{11}{Nadaraya-Watson Estimator}{equation.3.19}{}}
\newlabel{cond}{{3.20}{11}{Kernel Projection Estimators}{equation.3.20}{}}
\newlabel{toto}{{3.22}{11}{Fokkerâ€“Planck and PDE-Based Approximation}{equation.3.22}{}}
\citation{PPO}
\citation{DQN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Heuristic-controlled Learning}{12}{subsection.3.6}\protected@file@percent }
\newlabel{Heuristic-controlled Learning}{{3.6}{12}{Heuristic-controlled Learning}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{12}{section.4}\protected@file@percent }
\newlabel{Experiments}{{4}{12}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup and Kernel Configuration}{12}{subsection.4.1}\protected@file@percent }
\citation{HeuristicLunar}
\citation{FuMePrNaSh}
\citation{sutton2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Cartpole}{13}{subsection.4.2}\protected@file@percent }
\newlabel{Cartpole}{{4.2}{13}{Cartpole}{subsection.4.2}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:image1}{{1a}{13}{Mean cumulative reward per episode}{figure.caption.22}{}}
\newlabel{sub@fig:image1}{{a}{13}{Mean cumulative reward per episode}{figure.caption.22}{}}
\newlabel{fig:image2}{{1b}{13}{Mean Cumulative time per episode}{figure.caption.22}{}}
\newlabel{sub@fig:image2}{{b}{13}{Mean Cumulative time per episode}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Lunar-Lander}{13}{subsection.4.3}\protected@file@percent }
\newlabel{Lunar-Lander}{{4.3}{13}{Lunar-Lander}{subsection.4.3}{}}
\newlabel{fig:LNR}{{2a}{14}{Mean cumulative reward per episode}{figure.caption.23}{}}
\newlabel{sub@fig:LNR}{{a}{14}{Mean cumulative reward per episode}{figure.caption.23}{}}
\newlabel{fig:LNT}{{2b}{14}{Mean Cumulative time per episode}{figure.caption.23}{}}
\newlabel{sub@fig:LNT}{{b}{14}{Mean Cumulative time per episode}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{14}{section.5}\protected@file@percent }
\newlabel{Conclusion}{{5}{14}{Conclusion}{section.5}{}}
\bibcite{Chowdhury2017proceedings}{1}
\bibcite{Cuturi:2013}{2}
\bibcite{Mania2018}{3}
\bibcite{ChengHC2021}{4}
\bibcite{mnih2015}{5}
\bibcite{vanhasselt2016}{6}
\bibcite{Reinforce}{7}
\bibcite{SF2023}{8}
\bibcite{PLF-JMM-Wilmott}{9}
\bibcite{codpy}{10}
\bibcite{LeMeMi:2024}{11}
\bibcite{LeMe:2017}{12}
\bibcite{watkins1992}{13}
\bibcite{FuMePrNaSh}{14}
\bibcite{DQN}{15}
\bibcite{sutton2018}{16}
\bibcite{BTA}{17}
\bibcite{silver2014}{18}
\bibcite{PPO}{19}
\bibcite{ormoneit2002}{20}
\bibcite{engel2003}{21}
\bibcite{Xu2007}{22}
\bibcite{Jon2023}{23}
\bibcite{HeuristicLunar}{24}
\bibcite{DLR1}{25}
\bibcite{DLR2}{26}
\bibcite{DLR3}{27}
\bibcite{DLR4}{28}
\bibcite{DLR5}{29}
\citation{Jon2023}
\@writefile{toc}{\contentsline {section}{\numberline {A}APPENDIX}{16}{appendix.A}\protected@file@percent }
\newlabel{APPENDIX}{{A}{16}{APPENDIX}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}The softmax function}{16}{subsection.A.1}\protected@file@percent }
\newlabel{eq:softmax}{{A.1}{16}{The softmax function}{equation.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}A clustering methodology using kernel baseline RL algorithms}{16}{subsection.A.2}\protected@file@percent }
\newlabel{SEK}{{A.2}{16}{A clustering methodology using kernel baseline RL algorithms}{subsection.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}PPO under Standard Training Protocol}{17}{subsection.A.3}\protected@file@percent }
\newlabel{PPO_app}{{A.3}{17}{PPO under Standard Training Protocol}{subsection.A.3}{}}
\newlabel{fig:PPOLNR}{{3a}{17}{Mean cumulative reward per episode}{figure.caption.25}{}}
\newlabel{sub@fig:PPOLNR}{{a}{17}{Mean cumulative reward per episode}{figure.caption.25}{}}
\newlabel{fig:PPOLNT}{{3b}{17}{Mean Cumulative time per episode}{figure.caption.25}{}}
\newlabel{sub@fig:PPOLNT}{{b}{17}{Mean Cumulative time per episode}{figure.caption.25}{}}
\gdef \@abspage@last{17}
